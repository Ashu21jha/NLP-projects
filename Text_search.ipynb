{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Search Model for CDC -  NLP:\n",
        "\n",
        "### Problem Statement:\n",
        "\n",
        "You are a data scientist at the Centers for Disease Control and Prevention (CDC). You will take a critical part in the development of a new plan to prevent, control, and respond to future pandemics; launched by the Department of Health and Human Services. Your goal is to build an efficient and smart search engine for fast access to the CDC’s document database. To do this, you need to research and develop strategies to aggregate and quickly search historical and likely unstructured text data about earlier pandemics, so that they can be easily accessed when needed."
      ],
      "metadata": {
        "id": "To0c_exugD8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eOmCC-6hfuRH"
      },
      "outputs": [],
      "source": [
        "import spacy \n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating spacy model\n",
        "model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "eGjVH8_rgCky"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the json file\n",
        "with open('/content/drive/MyDrive/data.json',\n",
        "          'r') as outfile:\n",
        "          summaries = json.load(outfile)\n",
        "print(summaries[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiD-fNE0gcQx",
        "outputId": "58a0656f-1149-4409-80fb-d2fe725ca88e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['title', 'text', 'url'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Data:"
      ],
      "metadata": {
        "id": "TDF1D1I2hDwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = summaries[1]['text']"
      ],
      "metadata": {
        "id": "OyjCXJcsg5LB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lower-casing the data in the text\n",
        "# exploring the attributes of each token\n",
        "text_tokenized = model(text.lower())\n",
        "for token in text_tokenized[:5]:\n",
        "    print(type(token),\n",
        "          token.text, token.pos_,\n",
        "          token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XfKw7M5hK2C",
        "outputId": "ad9b0f30-949b-45cf-d490-e5b58c10f6d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.token.Token'> hiv PROPN nmod\n",
            "<class 'spacy.tokens.token.Token'> / SYM punct\n",
            "<class 'spacy.tokens.token.Token'> aids PROPN nsubjpass\n",
            "<class 'spacy.tokens.token.Token'> , PUNCT punct\n",
            "<class 'spacy.tokens.token.Token'> or CCONJ cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unclassified_tokens = [(\n",
        "    token.lemma_, token.dep_)\n",
        "    for token in text_tokenized if\\\n",
        "    token.dep_ == '']\n",
        "unclassified_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQJ0Ye64hraM",
        "outputId": "c687e3c5-ef10-4f2a-c0b5-4224606519c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' ', ''), ('\\n', ''), ('\\n', '')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing stopwords and punctuation:"
      ],
      "metadata": {
        "id": "ZZGrYfT5jJHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_without_sw = [word for word in\\\n",
        "                     text_tokenized if not\\\n",
        "                     word.is_stop and not\\\n",
        "                     word.is_punct]\n",
        "tokens_without_sw[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFxsHvOGipug",
        "outputId": "5df8ad82-d3df-4141-a798-6f2e30f014ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[hiv,\n",
              " aids,\n",
              " human,\n",
              " immunodeficiency,\n",
              " virus,\n",
              " considered,\n",
              " authors,\n",
              " global,\n",
              " pandemic,\n",
              " currently]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the texts"
      ],
      "metadata": {
        "id": "K9XDz_6Ijiaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_lemmas =[token.lemma_ for\\\n",
        "               token in tokens_without_sw\\\n",
        "               if token.dep_]\n",
        "token_lemmas[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oaER3j9jbZe",
        "outputId": "570ab96a-d1d9-4c96-8c45-c6e18ea0a84c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hiv',\n",
              " 'aids',\n",
              " 'human',\n",
              " 'immunodeficiency',\n",
              " 'virus',\n",
              " 'consider',\n",
              " 'author',\n",
              " 'global',\n",
              " 'pandemic',\n",
              " 'currently']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(document):\n",
        "    text_lowercased = model(document.lower())\n",
        "    tokens_without_stopwords = [word for word\\\n",
        "                                in text_lowercased if\\\n",
        "                                not word.is_stop and\\\n",
        "                                not word.is_punct]\n",
        "    token_lemmatized = [token.lemma_ for\\\n",
        "                        token in tokens_without_stopwords\\\n",
        "                        if token.dep_]\n",
        "    return token_lemmatized"
      ],
      "metadata": {
        "id": "Rh3DNDoNj12g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in summaries:\n",
        "    doc['tokenized_text'] = tokenizer(doc['text'])"
      ],
      "metadata": {
        "id": "ZYNFBjQtk98c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[0]['tokenized_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pZme8nnlEB4",
        "outputId": "9fe8082b-2da3-4f00-9a89-bc5d1d599f7d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pandemic',\n",
              " 'greek',\n",
              " 'πᾶν',\n",
              " 'pan',\n",
              " 'δῆμος',\n",
              " 'demos',\n",
              " 'people',\n",
              " 'epidemic',\n",
              " 'infectious',\n",
              " 'disease',\n",
              " 'spread',\n",
              " 'large',\n",
              " 'region',\n",
              " 'instance',\n",
              " 'multiple',\n",
              " 'continent',\n",
              " 'worldwide',\n",
              " 'affect',\n",
              " 'substantial',\n",
              " 'number',\n",
              " 'people',\n",
              " 'widespread',\n",
              " 'endemic',\n",
              " 'disease',\n",
              " 'stable',\n",
              " 'number',\n",
              " 'infected',\n",
              " 'people',\n",
              " 'pandemic',\n",
              " 'widespread',\n",
              " 'endemic',\n",
              " 'disease',\n",
              " 'stable',\n",
              " 'number',\n",
              " 'infected',\n",
              " 'people',\n",
              " 'recurrence',\n",
              " 'seasonal',\n",
              " 'influenza',\n",
              " 'generally',\n",
              " 'exclude',\n",
              " 'occur',\n",
              " 'simultaneously',\n",
              " 'large',\n",
              " 'region',\n",
              " 'globe',\n",
              " 'spread',\n",
              " 'worldwide',\n",
              " 'human',\n",
              " 'history',\n",
              " 'number',\n",
              " 'pandemic',\n",
              " 'disease',\n",
              " 'smallpox',\n",
              " 'tuberculosis',\n",
              " 'fatal',\n",
              " 'pandemic',\n",
              " 'record',\n",
              " 'history',\n",
              " 'black',\n",
              " 'death',\n",
              " 'know',\n",
              " 'plague',\n",
              " 'kill',\n",
              " 'estimate',\n",
              " '75–200',\n",
              " 'million',\n",
              " 'people',\n",
              " '14th',\n",
              " 'century',\n",
              " 'term',\n",
              " 'later',\n",
              " 'pandemic',\n",
              " 'include',\n",
              " '1918',\n",
              " 'influenza',\n",
              " 'pandemic',\n",
              " 'spanish',\n",
              " 'flu',\n",
              " 'current',\n",
              " 'pandemic',\n",
              " 'include',\n",
              " 'covid-19',\n",
              " 'sars',\n",
              " 'cov-2',\n",
              " 'hiv',\n",
              " 'aids']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('summaries.josn','w') as outfile:\n",
        "    json.dump(summaries, outfile)"
      ],
      "metadata": {
        "id": "N_Ib2dbmlHkL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "T3dBHvRemFYs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/summaries.josn','r') as outfile:\n",
        "    summaries = json.load(outfile)"
      ],
      "metadata": {
        "id": "ma1y1d9emhj9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bulding a Vocabulary corpus:"
      ],
      "metadata": {
        "id": "OCL-K5oam6lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenating all tokenized texts into a simble text:\n",
        "tokenized_texts = [i['tokenized_text'] for i in summaries]\n",
        "\n",
        "# flattening the list of lists\n",
        "vocab = list(itertools.chain(*tokenized_texts))\n",
        "\n",
        "# removing duplicates\n",
        "vocab = list(set(vocab))"
      ],
      "metadata": {
        "id": "2MrGPq63mriE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vocab.json','w') as outfile:\n",
        "    json.dump(vocab, outfile)"
      ],
      "metadata": {
        "id": "GyPeS1qinnK8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the appearance of tokens in the document\n",
        "docs_token_counter = []\n",
        "for doc in summaries:\n",
        "    doc_tokenized = doc['tokenized_text']\n",
        "    docs_token_counter.append(Counter(doc_tokenized))"
      ],
      "metadata": {
        "id": "NweWiAy_n_UU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding All Unique tokens in the corpus:"
      ],
      "metadata": {
        "id": "PClk76ehpJP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_docs_with_token = {}\n",
        "for token in vocab:\n",
        "    count_docs = sum([1 for doc in\\\n",
        "        docs_token_counter if token\\\n",
        "        in doc.keys()])\n",
        "    number_docs_with_token[token] = count_docs"
      ],
      "metadata": {
        "id": "cbStpYH_o82v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_docs_with_token['pandemic']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1D3O1gLqCOw",
        "outputId": "fc5e312d-6df7-4258-c9a6-dfa961649bc6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the TF-IDF of the document:"
      ],
      "metadata": {
        "id": "j1L4omG_qqIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(docs_token_counter):\n",
        "    doc_length = len(doc)\n",
        "    tfidf_vec = []\n",
        "    for token in vocab:\n",
        "        \n",
        "        #computing the term frequency per document\n",
        "        tf = doc[token] / len(summaries[i]['tokenized_text'])\n",
        "\n",
        "        # computing the log of inverse document\n",
        "        idf = np.log(len(summaries)/ number_docs_with_token[token])\n",
        "\n",
        "        tfidf = tf*idf\n",
        "        tfidf_vec.append(tfidf)\n",
        "\n",
        "    #add tfidf vector to dictionaries\n",
        "    summaries[i]['tf_idf'] = tfidf_vec"
      ],
      "metadata": {
        "id": "tnEERGKXqFqS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('summaries.json','w') as json_file:\n",
        "    json.dump(summaries, json_file)"
      ],
      "metadata": {
        "id": "iyt-0Ks5rzPQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'highest pandemic casualty'"
      ],
      "metadata": {
        "id": "_Epzhx8B0N4g"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(document):\n",
        "    text_lowercased = model(document.lower())\n",
        "    tokens_without_stopwords = [word for word\\\n",
        "                                in text_lowercased if\\\n",
        "                                not word.is_stop and\\\n",
        "                                not word.is_punct]\n",
        "    token_lemmatized = [token.lemma_ for\\\n",
        "                        token in tokens_without_stopwords\\\n",
        "                        if token.dep_]\n",
        "    return token_lemmatized"
      ],
      "metadata": {
        "id": "mF_48XohsQKj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(query, vocab=vocab):\n",
        "    query_tokenizer = tokenizer(query)\n",
        "    query_token_counter = Counter(query_tokenizer)\n",
        "    query_vec = []\n",
        "    for token in vocab:\n",
        "        tf = query_token_counter[token] / len(query_token_counter)\n",
        "        idf = np.log(len(summaries) / number_docs_with_token[token])\n",
        "        tfidf = tf*idf\n",
        "        query_vec.append(tfidf)\n",
        "\n",
        "    return query_vec"
      ],
      "metadata": {
        "id": "q9U30ndcse5K"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching Documents using Scikit-Learn:"
      ],
      "metadata": {
        "id": "IztvnV2vv4N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buding a search function\n",
        "def search_tfidf(query, docs):\n",
        "\n",
        "    # vectorize query\n",
        "    query_vec = vectorizer(query)\n",
        "    query_arr = np.array(query_vec)\n",
        "    rankings =[]\n",
        "    for doc in docs:\n",
        "        doc_rank = {}\n",
        "        doc_arr = np.array(doc['tf_idf'])\n",
        "        rank = cosine_similarity(\n",
        "            query_arr.reshape(1,-1),doc_arr.reshape(1,-1))[0][0]\n",
        "        if rank >0:\n",
        "            doc_rank['title'] = doc['title']\n",
        "            doc_rank['rank'] = rank\n",
        "            rankings.append(doc_rank)\n",
        "    return sorted(rankings,\n",
        "                  key=lambda k: k['rank'],\n",
        "                  reverse=True)\n",
        "rankings = search_tfidf(query, summaries)"
      ],
      "metadata": {
        "id": "kl5yzloQvooX"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_tfidf('ebola', summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAQoz9hnyGHX",
        "outputId": "76f648ec-b482-496f-ef1e-2402ea75981b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rank': 0.11754261855142299, 'title': 'Plague of Cyprian'},\n",
              " {'rank': 0.071125289564604, 'title': 'Science diplomacy and pandemics'}]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check if the article 'Plague of Cyprian' has a word \"ebola\" in it\n",
        "for s in summaries:\n",
        "    if s[\"title\"] == 'Plague of Cyprian':\n",
        "        print(s[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-xNVLKLzp5p",
        "outputId": "e4a79ba2-6ca3-4dd1-f543-3b2f976de1ce"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262. The plague is thought to have caused widespread manpower shortages for food production and the Roman army, severely weakening the empire during the Crisis of the Third Century. Its modern name commemorates St. Cyprian, bishop of Carthage, an early Christian writer who witnessed and described the plague. The agent of the plague is highly speculative because of sparse sourcing, but suspects have included smallpox, pandemic influenza and viral hemorrhagic fever (filoviruses) like the Ebola virus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and inverted index:\n"
      ],
      "metadata": {
        "id": "CPoxGSOjBDLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    inverted_index[word] = []\n",
        "    for doc in summaries:\n",
        "        # for each word in corpus vocabulary list all articles\n",
        "        # it occurs in and this word's TfIdf score for this article\n",
        "        if doc['tf_idf'][i]!=0:\n",
        "            inverted_index[word].append((doc['title'], doc['tf_idf'][i])) \n",
        "\n",
        "# Now you have a lookup table of all articles that have a particular keyword\n",
        "# lets request a list of articles with the word \"coronavirus\" in them\n",
        "inverted_index[\"coronavirus\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p09iy_-X4RRa",
        "outputId": "ab48d0ae-ae23-4bab-f375-0fca41bd0dca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('COVID-19 pandemic', 0.05749582125920263)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if 'coronavirus' is in the article\n",
        "for s in summaries:\n",
        "    if s['title'] =='COVID-19 pandemic':\n",
        "        print(s['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A8YohzxBpe4",
        "outputId": "7ae73cca-8526-48d4-d173-c50dd1e9dd6f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 17 October 2020, more than 39.5 million cases have been confirmed, with more than 1.1 million deaths attributed to COVID-19.\n",
            "\n",
            "Common symptoms include fever, cough, fatigue, breathing difficulties, and loss of smell. Complications may include pneumonia and acute respiratory distress syndrome. The incubation period is typically around five days but may range from one to 14 days. There are several vaccine candidates in development, although none have proven their safety and efficacy. There is no known specific antiviral medication, so primary treatment is currently symptomatic.\n",
            "Recommended preventive measures include hand washing, covering mouth or wearing face mask when sneezing or coughing, social distancing, disinfecting surfaces, ventilation and air-filtering, and monitoring and self-isolation if exposed or symptomatic. Travel restrictions, lockdowns, workplace hazard controls, and facility closures have been implemented. Many places have also worked to increase testing capacity and trace contacts of the infected. These have caused social and economic disruption, including the largest global recession since the Great Depression. Extreme poverty and global famines are affecting hundreds of millions, inflamed by supply shortages. Many events, the environment and education systems have also been affected. Misinformation about the virus has circulated globally. There have been many incidents of xenophobia and racism against Chinese people and against those perceived as being Chinese or as being from areas with high infection rates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching inverted Index:"
      ],
      "metadata": {
        "id": "KVxWWk6UCTsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "9yPUKkGDDLHz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the tokenizer from Milestone 1 to tokenize search queries\n",
        "\n",
        "def tokenizer(document):\n",
        "    text_lowercased = model(document.lower())\n",
        "    tokens_without_stopwords = [word for word \n",
        "                     in text_lowercased \n",
        "                     if not word.is_stop \n",
        "                     and not word.is_punct\n",
        "                     and len(word.dep_.strip())!=0]   \n",
        "    \n",
        "    token_lemmatized = [token.lemma_ \n",
        "               for token\n",
        "               in tokens_without_stopwords]\n",
        "    \n",
        "    return token_lemmatized"
      ],
      "metadata": {
        "id": "C-6Fu55ADefL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index =inverted_index):\n",
        "    query_tokens = tokenizer(query)\n",
        "    new_list =[]\n",
        "    for token in query_tokens:\n",
        "        new_list.extend(inverted_index[token])\n",
        "    \n",
        "    output = defaultdict(int)\n",
        "    for k,v in new_list:\n",
        "        output[k] += v\n",
        "    results = [(x,y) for x,y in output.items()]\n",
        "    \n",
        "    return sorted(results, \n",
        "                  key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "wK8_UWXbB4R7"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title, score = search(query='world health organiZation')[0]\n",
        "for s in summaries:\n",
        "    if s['title'] ==title:\n",
        "        print(s['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEhKig8ECwyO",
        "outputId": "50583888-572e-4647-f6c9-bd2dab351807"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Johns Hopkins Center for Health Security (abbreviated CHS; previously the UPMC Center for Health Security, the Center for Biosecurity of UPMC, and the Johns Hopkins Center for Civilian Biodefense Strategies) is an independent, nonprofit organization of the Johns Hopkins Bloomberg School of Public Health, and part of the Environmental Health and Engineering department. It is concerned with the areas of health consequences from epidemics and disasters as well as averting biological weapons development, and implications of biosecurity for the bioeconomy. It is a think tank that does policy research and gives policy recommendations to the United States government as well as the World Health Organization and the UN Biological Weapons Convention.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search(query='Ebola virus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qUJqfE1ETzd",
        "outputId": "9ddc2ec0-9f7d-45de-b039-75a7e970524c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Virus', 0.06746676589985189),\n",
              " ('Plague of Cyprian', 0.0634287152349009),\n",
              " ('Crimson Contagion', 0.0339553131009123),\n",
              " ('Viral load', 0.03386619154421699),\n",
              " ('Disease X', 0.031470777995967494),\n",
              " ('Swine influenza', 0.028050041257275376),\n",
              " ('Science diplomacy and pandemics', 0.027286695292144007),\n",
              " ('HIV/AIDS in Yunnan', 0.022837201731587032),\n",
              " ('HIV/AIDS', 0.013653988336874786),\n",
              " ('Spanish flu', 0.012903018978346673),\n",
              " ('Epidemiology of HIV/AIDS', 0.005973619897382719),\n",
              " ('COVID-19 pandemic', 0.005060007442488892)]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in summaries:\n",
        "    if s[\"title\"] == 'Crimson Contagion':\n",
        "        print(s[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i--rNmriEgI6",
        "outputId": "2bd0aaa2-6390-472c-8b39-bbc9d94d74fe"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crimson Contagion was a joint exercise conducted from January to August 2019, in which numerous national, state and local, private and public organizations in the US participated, in order to test the capacity of the federal government and twelve states to respond to a severe pandemic of influenza originating in China.\n",
            "The simulation, which was conducted months prior to the start of the COVID-19 pandemic, involves a scenario in which tourists returning from China spread a respiratory virus in the United States, beginning in Chicago. In less than two months the virus had infected 110 million Americans, killing more than half a million. The report issued at the conclusion of the exercise outlines the government's limited capacity to respond to a pandemic, with federal agencies lacking the funds, coordination, and resources to facilitate an effective response to the virus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TVPmadzMErRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}